{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scene Creation\n",
    "\n",
    "The aim of this notebook is to create a scene with chosen camera intrinsic and extrinsic parameters.\n",
    "\n",
    "A scene consists of the following elements:\n",
    "\n",
    "- Camera Frames\n",
    "- Bounding Box Labels\n",
    "- Center of Mass Ground Truth\n",
    "\n",
    "[*visualization*](https://lucid.app/lucidchart/a570a6c2-e5ef-412b-86fb-98b6d1292aa6/edit?viewport_loc=138%2C-10%2C1707%2C743%2C0_0&invitationId=inv_d45a706f-3778-4e32-a766-520a623f508f)\n",
    "\n",
    "\n",
    "***TODO:***\n",
    "- capire come trattare l'origine del sistema di riferimento SinD rispetto al nostro\n",
    "- cambiare il modo in cui definiamo la camera position\n",
    "- se azimuth ed elevation li calcoliamo a partire dai camera parameters e dalla posizione del centro della bounding box nell'image plane, perchè li dobbiamo usare nella regressione? -> vuol dire che dobbiamo cambiare la classe Dataset in modo che due parametri vengano presi dal dataset e due vengano calcolati e 4 elementi siano dati in input alla rete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scene Parameters Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Camera Intrinsic Parameters\n",
    "focal_length=0.0036\n",
    "sensor_size=(0.00367, 0.00274)\n",
    "image_size=(640, 480)\n",
    "kappa=0.4\n",
    "\n",
    "\n",
    "# Define Camera Position\n",
    "alpha, beta = random_view()                     # alpha, beta are respectively azimuth and elevation for the camera orientation\n",
    "camera_distance = random.uniform(20, 40)\n",
    "camera_position = camera_distance * np.array([\n",
    "    np.cos(beta) * np.cos(alpha),\n",
    "    np.cos(beta) * np.sin(alpha),\n",
    "    np.sin(beta)\n",
    "])\n",
    "\n",
    "# Define Camera Orientation: the camera always points at the origin of the world coordinates\n",
    "up = np.array([0, 0, 1])                   # define positive vertical direction\n",
    "from_point = camera_position        # define 'from' and 'to' points for the camera\n",
    "to_point = np.array([0, 0, 0])\n",
    "R_C2W, t_C2W = lookat(from_point, to_point, up)     # compute Rotation and Translation -> Transformation Matrix\n",
    "R_C2W = R_C2W @ matrix_from_axis_angle((1, 0, 0, np.pi))     # flips about axis 1 to obtain Camera Frame\n",
    "cam2world = transform_from(R_C2W, t_C2W)\n",
    "intrinsic_camera_matrix = np.array([                # create intrinsic camera matrix\n",
    "    [focal_length, 0, sensor_size[0] / 2],\n",
    "    [0, focal_length, sensor_size[1] / 2],\n",
    "    [0, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from SinD\n",
    "\n",
    "Dobbiamo prendere tutto quello che ci interessa e che ci servirà dopo per creare i labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Bounding Box Labels*\n",
    "\n",
    "It will generate the json file annotations, which will look something like this:\n",
    "\n",
    "```json\n",
    "\"annotations\": [\n",
    "        {\n",
    "            \"area\": 4.292386303916224,\n",
    "            \"bbox\": [\n",
    "                1217.57,\n",
    "                335.11,\n",
    "                8.43,\n",
    "                10.35\n",
    "            ],\n",
    "            \"category_id\": 2,\n",
    "            \"id\": 163514,\n",
    "            \"image_id\": 1,\n",
    "            \"iscrowd\": 0,\n",
    "            \"point\": [\n",
    "                270.0,\n",
    "                75.0\n",
    "            ],\n",
    "            \"segmentation\": []\n",
    "        },\n",
    "```\n",
    "\n",
    "Qui si userà ```generate_box()``` e tutto il resto per generare le bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prendi la posizione\n",
    "- prendi la dimensione\n",
    "- prendi l'orientamento\n",
    "\n",
    "-> hai i vertici della box nelle world coordinates\n",
    "\n",
    "- converti in 2D coordinates on image plane\n",
    "- crea bounding box on image plane con `create_bounding_box()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Save Trajectory Ground Truth*\n",
    "\n",
    "It will save the trajectory of the center of mass from SinD data. There are two remarks:\n",
    "\n",
    "- it could be included into the json file above\n",
    "- it needs to include some sort of id identifiers in the case of multiple vechiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Blender Rendering*\n",
    "\n",
    "It will run the python script to load the scene in Blender (setting keyframes, camera parameters, etc.) and render the images.\n",
    "\n",
    "It will all be done by running python script in Blender via CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Che ci deve stare nelle annotations?***\n",
    "\n",
    "per ogni frame:\n",
    "\n",
    "- object_id\n",
    "- class_id -> ['car', 'truck', 'bus', 'motorcycle', 'bicycle', 'pedestrian']\n",
    "- bbox -> [x, y, w, h]\n",
    "\n",
    "<frame_id> <object_id> <bbox_x> <bbox_y> <bbox_width> <bbox_height> <class_id> <conf> <xc_true> <yc_true> <xw_true> <yw_true> <zw_true> <yaw_rad> ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macchinine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
